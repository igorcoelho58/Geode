VÍDEO #2
======================================================================
Título: Python Image Captioning Tutorial | Image To Text Blip Python Guide
URL: https://youtube.com/watch?v=CzO8VuaHfKM
Idioma: pt (Whisper IA) (whisper-ai)
Tamanho: 15277 caracteres
Data Coleta: 09/01/2026 12:22
======================================================================

 In today's video, we'll be taking a look at how to caption images using Python. We're going to be using the blip model coupled with Hagenface Transformer Library to achieve this functionality. And we're going to be building a web application that is going to present us with a UI that we're going to be building using Gradio. And then to this UI, we can pass in an image and then submit our query. And it's going to basically give us a description of what it thinks is within the image. So in this case, we can see that I gave an image of the summit of Mount Everest. And I got a response that details to me that the picture shows the summit of Mount Everest. I can put in another picture here, for example, this picture of a man with his dog, submit this and it's going to go ahead and analyze this picture. And then let me know that this picture of a man sitting on a bench with a dog. So this is going to be a spectacular video and you're going to learn a lot about image captioning. So to get started, the first thing that we're going to be doing is actually taking a look at the dependencies that we're going to be using to actually build our Python. It's weird. The first package that we're going to be using is going to be called Pillow. And we're going to be using Pillow as the actual image library that we're going to use to actually load our image and then pass it to the actual blade model. So to install Pillow, we'll be just using the pip install Pillow command. So I'll copy this to the clipboard, come back and paste this in within my terminal. And I'm going to be installing this package within a virtual environment that I've created specific to this project. Once Pillow has been installed, the next thing that I'm going to be doing is installing the actual transformers library, which is provided to us by hugging face. And it's going to make it very easy for us to actually work with the actual blip model. So let's copy the actual pip install command, come back, paste in pip install transformers, and I'll wait for the actual transformers library to finish installing. While this is going on, I'd like to quickly let you know that links to all of the resources that I mentioned within this video as well as a link to the source could can be found in the description below. So feel free to take a look at it if you're confused at any point. And if you want to learn more about what the blip model is, then I'll leave a link to that down in the description as well as well as a link to the actual model that we're going to be using, which is blip image captioning base provided to us by Salesforce. But in a very quick pinch, blip is a model that is able to perform various multimodal tasks. And this includes the ability to visually question answer image text retrieval and image captioning. And today we're going to be taking a look at image captioning. So with this set, let's call back and let's see if transformers has been installed. Yes, it is. So with that done, the next thing that we're going to be doing is actually installing PyTorch on our system. To install PyTorch, I'll leave a link down in the description below, but I'll take you to the actual PyTorch website. And from here, what you'll do is basically come to the point where it says start locally and basically select the options that correspond to the system that you're going to be running this script on. In my case, I'm going to be running it on a map. So I'll do stable, Mac, pip, Python and default. And it's going to spit out the command that I need to use in order to install PyTorch properly. So I'll copy this command, come back and then paste this in. And then I'll wait for this to complete. And once this is complete, then I will resume the video. Welcome back everybody. So now that PyTorch is installed, the next thing that I'm going to be doing is installing Gradio. And Gradio is an open source Python package that allows us to quickly build or demo web applications that can interact with various machine learning model APIs or any arbitrary Python function. And we're going to be using this to create the UI that I had shown you in the previous step while I was showing you the demo of the actual script that we're going to be building. So to install Gradio, what you can do is actually come to the quick start guide link to this will be in the description as well. Copy the pip install Gradio command, taste it in and then I'll just wait for this to complete. And once Gradio is installed, that's pretty much all of the packages that we required. So once this is done, the next thing that you're going to be doing is actually creating the Python script which is going to handle all of this functionality. In my case, I'm going to call my Python script the main.py. And once this is done, the next thing that I'm going to be doing is actually important the packages that are just installed. So firstly, I'm going to import the pillow package and I'm going to say from pill import image since we only require that. Once this is done, we're also going to be from Transformers importing the blip processor. And then besides this, I am also going to be importing a Firemember, correctly blip for conditional generation. And once this is done, the last thing that I'm going to be doing is importing Gradio as GR. And once this is done, we can move to the next step. So the first thing that we have to do is basically create a processor. Well, to create a processor, I'll create a variable and I'll say it's going to be processor and it's going to be equal to blipprocessor. And then from pre-trained. And then here, what we're going to be doing is actually passing it the model name. So what is the model name going to be? Well, the model name in this case is going to be Salesforce, forward slash, and then what the name of the model is, which is blip-image-captioning-base. If you want to know where I got this model name from, you can see that it's highlighted here on hugging face. It's Salesforce slash blip slash image slash captioning-slashbase. If you can't do this, then what you can do is come to the actual section where it says use this model and then say Transformers as the library. And it's going to show you how do we actually do this. So now that this done, the next thing that we have to do, now that we've created our processor is actually create our model. So how are we going to be doing that? Well for that, again, I am going to be creating another variable and I'm going to be calling this model. And I'm going to say this is going to be called the blipfor conditional generation dot from pre-trained. And then again, we're going to be giving it the name of the model we'd like to use. So this will be the same as the one we had used before for the processor. And with that, that's pretty much all we have to do. So now that we've created the processor and the model, the next thing that I'm going to be doing is quickly running the script and ensuring that there are no errors. And since this is the first time that we're running the script, it's going to go ahead and basically download these models and then cache them under system. So it might take a bit time for this actual process to complete. It just depends upon the speed of your internet connection. So I'm going to do Python and then main dot p y to actually run the script. And I'll wait for the actual download process to complete and then I will resume the video. Welcome back everybody. So the actual process completed. And as you can see, I exited out of the execution of the script without any errors. So this means basically the setup for our processor and model was good. So now what we can do is actually create a function that is going to be responsible for actually taking in an image and then doing all of the processing logic on it, passing it to the model, etc, etc. and then giving us an output for what it deduces the actual content within the image it or what the caption for the images. So how are we going to be doing this? Well, I am going to be creating a function and I'm going to say that this is going to be called generate underscore caption. And then I'll say that this is going to have an image passed to it. Within this function, the first thing that we're going to be doing is basically converting the image that gets passed to us into an array to do that. We're going to be using the image dot from array function. And then here we can pass in the actual image. Once this is done, the next thing that I'm going to be doing is for now just printing the image underscore input like so. Like we can just return an empty string for now because I want to show you what's going to happen once we actually run this function. So now that this is done, the next thing that I'm going to be doing is actually creating the Gradio interface that is going to allow us to create some UI and that link that UI to this actual function. So to do that, I will say that I'm going to create a variable called demo and then I'm going to say that this is going to be set to gr dot interface. So we're creating a new Gradio interface. Our interface is going to link up to a function and the function is going to be generate caption. Then our interface is going to have one input. So I'm going to specify it within the inputs list and the input is going to be of type gr dot image. So we're going to have an input of type image and the label for this is going to be image. There we go. So now that this is done, the next thing that I'm going to be doing after this is then specifying the outputs. So what are the outputs of our interface? So inputs basically refer to all of the inputs that we're going to be giving to our function and Gradios automatically going to generate all of the UI components for us based upon the types of inputs we define and output basically means what type of an output then we're going to be getting from from our actual function and what type of an actual output interface we're going to be using to actually then display that output. So in this case, we know that our output is going to be of type text since we're returning the string. So what I can do is say outputs is equals to an array and within this I can do GR dot m and text. So there are a bunch of different components. We're going to be just using the text one and then with this you can do label is equals to m and we can do caption. There we go. So now that this is done, the next thing that I'm going to be doing is going ahead and then basically launching the demo. So for that I can do demo dot launch and this will basically take care of everything for us. This includes firstly setting up the web server and creating our interface and then giving us the URL through which we can access that interface. So when this done, I'm going to do Python, main.py, press enter and I'll wait for the actual script to finish its execution and then spit out an actual URL to me which I can go to and then access the actual Gradio instance that's running. So there we go. Now we can see that it says that we can access the UI on 127.0.0.1 port 7860 so let's go there and there we go. The UI is being displayed to us and this is the part of Gradio with just a few lines of code we were able to define a beautiful interface that gives us an input image and then basically links that up with that function and once our function completes its execution, whatever the return value is, it's shown to us here in the captions field. So now if I go ahead and give an image, so let's actually drag and drop this image in. Nothing's going to happen for now but if I click submit, you are going to see that after some time it's going to spit out nothing but here we can see that it gives me an output in the console saying that the actual image input is in the form of a pillow image so we can move on to the next step. So for the next step what I need to do is basically define the inputs that I'm going to pass to my actual blip model. So the inputs need to be in a specific form because the model only understands a specific form and that's where we're going to be using our processor. So what I'm going to do is say inputs is equals to, so these are the inputs we're going to give to our model and they're going to be equal to the processor that we had created and to the processor we'll pass in our image as well as saying hey processor take this image and then convert that into the appropriate form that our model can then ingest and then I'm also going to define the return tensors type which is going to be equal to PAT. So this means that the tensors that are returned they need to be in the form of pie torch. So once this is done that I'm going to do the following which is to actually take these inputs and run the model on it. So I'm going to do model dot generate and then give it the inputs star star inputs there we go and then I'm basically going to save the output of the model dot generate function in the out variable. And once this is done the last thing that we are going to be doing and is just printing out there we go. So with this done let's control C to exit out of the script that was running python main dot PY once again to restart script and now that the script is running once again let's reload our web UI and then let's drag and drop the image once again click submit and as you can see we get this tensor that's printed to the actual console. So now the model is actually given us an output but the issue is that the output currently is in a vectorized form. So what we need to do is basically decode it. So how are we going to be decoding it well to decode the output what I can do is create a variable called caption and then set that equal to and I am going to do processor. So the same way we use the processor to encode the actual input and then feed it to our model. So now in the same manner we're going to be using the processor to then decode the actual tensor that's given to us into a human readable form. So to do that I will do processor dot decode and then I know that the actual tensor is in the 0 index. So there we go we do that and then after this I'm going to do skip special tokens equals to true and that's pretty much all we have to do and now what we can do is return the caption. There we go. So with this let's do command save, control the exit out python main dot p y and then once the script actually starts up once again I am going to go ahead and restart the reload the web UI drag and drop the image in click submit and there we go it says it's a picture of a man sitting on a bench with a dot. So let's test it again with the other image that I had used. So let's drag and drop that in submit and it says that it's the summit of mold ever with. So with that that's pretty much it for today's video. I hope that you enjoyed this video and learned a thing or two about how image captioning works with python and how you can implement it for yourself. As always if you enjoyed the video then please don't forget to leave a like on the video and subscribe to my channel so that you get notified every time you release a new video and with that stay happy stay healthy keep learning keep growing and I'll see you guys in the next video. Bye bye.